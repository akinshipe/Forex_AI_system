{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "37ccab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from keras.datasets import mnist\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "id": "fde693da",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_list = [#'USDCHF10080',\n",
    "                 #'GBPUSD10080', 'EURUSD10080', \n",
    "    #'USDJPY10080', \n",
    "    #'USDCAD10080', \n",
    "    #'AUDUSD10080', \n",
    "    #'NZDUSD10080',\n",
    "                 #'GBPCHF10080',\n",
    "    #'EURCHF10080', \n",
    "    #'CHFJPY10080', \n",
    "    #'CADCHF10080',\n",
    "    #'AUDCHF10080', \n",
    "    #'NZDCHF10080', \n",
    "    'EURGBP10080',\n",
    "             #   'GBPCAD10080',\n",
    "     #'GBPAUD10080', \n",
    "    #'EURJPY10080',\n",
    "    #'EURCAD10080',\n",
    "    #'EURAUD10080',\n",
    "    #'EURNZD10080',\n",
    "    #'CADJPY10080', \n",
    "    #'AUDJPY10080',\n",
    "    #'NZDJPY10080',\n",
    "    #'AUDCAD10080', \n",
    "    #'NZDCAD10080', \n",
    "                #'AUDNZD10080'\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "# for q in currency_list:\n",
    "    \n",
    "#     errors = []\n",
    "    \n",
    "#     for x in range(5):\n",
    "\n",
    "#         currency = q.replace('10080','')\n",
    "\n",
    "#         data = pd.read_excel('files/currency_training_data/' + currency +'_combine_data_dataframe.xlsx', sheet_name=0)\n",
    "#         #data = data.head(695)\n",
    "\n",
    "\n",
    "#         X = data.drop(columns=['Unnamed: 0', \n",
    "#                                'date_start',  'nextweek_class',\n",
    "\n",
    "\n",
    "#                               ])\n",
    "\n",
    "\n",
    "\n",
    "#         y = data['nextweek_class']\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 )\n",
    "\n",
    "\n",
    "\n",
    "#         lnr= LinearRegression()\n",
    "#         lnr.fit(X_train, y_train)\n",
    "#         y_predict = lnr.predict(X_test)\n",
    "        \n",
    "        \n",
    "#         error = sqrt(mean_squared_error(y_test, y_pred))\n",
    "#         errors.append(error)\n",
    "       \n",
    "        \n",
    "#     average_error = sum(errors)/len(errors)\n",
    "       \n",
    "#     print(q + \" Linear regression Average \" + str(average_error))\n",
    "    \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "id": "1fcc8706",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 7s 13ms/step - loss: 151.6380\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 1s 15ms/step - loss: 151.6371\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 151.6330\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 151.6332\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 1s 14ms/step - loss: 151.6452\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 158.3248\n",
      "------------------------------------------------------------------------------------ 0\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 4s 10ms/step - loss: 153.1636\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.1600\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 153.1595\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.1632\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.1562\n",
      "7/7 [==============================] - 1s 6ms/step - loss: 149.7332\n",
      "------------------------------------------------------------------------------------ 1\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 5s 10ms/step - loss: 149.5457\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 149.5426\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 149.5408\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 149.5442\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 149.5442\n",
      "7/7 [==============================] - 2s 5ms/step - loss: 170.1739\n",
      "------------------------------------------------------------------------------------ 2\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 8s 10ms/step - loss: 152.9878\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 152.9820\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 152.9781\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 11ms/step - loss: 152.9701\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.9615\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 150.8544\n",
      "------------------------------------------------------------------------------------ 3\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 10ms/step - loss: 154.4422\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 154.4331\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 154.4374\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 154.4345\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 154.4235\n",
      "7/7 [==============================] - 1s 7ms/step - loss: 142.5463\n",
      "------------------------------------------------------------------------------------ 4\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 11ms/step - loss: 152.3235\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.3172\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.3149\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.3146\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.3120\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 154.5047\n",
      "------------------------------------------------------------------------------------ 5\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 10ms/step - loss: 151.9484\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 151.9462\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 151.9385\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 151.9375\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 151.9523\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 156.6913\n",
      "------------------------------------------------------------------------------------ 6\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 4s 10ms/step - loss: 154.3202\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 154.3183\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 154.3121\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 154.3031\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 154.2925\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 143.2329\n",
      "------------------------------------------------------------------------------------ 7\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 10ms/step - loss: 150.5650\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 150.5551\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 150.5512\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 150.5504\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 150.5391\n",
      "7/7 [==============================] - 1s 7ms/step - loss: 164.5308\n",
      "------------------------------------------------------------------------------------ 8\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 10ms/step - loss: 152.7863\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.7837\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.7807\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.7793\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 152.7771\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 151.8857\n",
      "------------------------------------------------------------------------------------ 9\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 4s 10ms/step - loss: 153.7850\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.7837\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.7739\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.7699\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.7441\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 146.3110\n",
      "------------------------------------------------------------------------------------ 10\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 10ms/step - loss: 158.8612\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 158.8554\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 158.8535\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 158.8477\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 158.8356\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 117.6876\n",
      "------------------------------------------------------------------------------------ 11\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 10ms/step - loss: 155.1398\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 155.1349\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 155.1351\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 155.1286\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 155.1257\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 138.6249\n",
      "------------------------------------------------------------------------------------ 12\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 10ms/step - loss: 152.7342\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.7380\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 152.7346\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 12ms/step - loss: 152.7306\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 152.7298\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 152.1376\n",
      "------------------------------------------------------------------------------------ 13\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38/38 [==============================] - 3s 10ms/step - loss: 156.9535\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 156.9502\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 156.9538\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 156.9493\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 156.9492\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 128.3808\n",
      "------------------------------------------------------------------------------------ 14\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 9ms/step - loss: 153.6027\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 153.6001\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 153.6046\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 153.5872\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 153.5861\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 147.2919\n",
      "------------------------------------------------------------------------------------ 15\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 9ms/step - loss: 149.1647\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 149.1638\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 149.1465\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 149.1449\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 149.1265\n",
      "7/7 [==============================] - 1s 4ms/step - loss: 172.4451\n",
      "------------------------------------------------------------------------------------ 16\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 9ms/step - loss: 151.1600\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 151.1569\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 151.1531\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 151.1548\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 151.1425\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 161.0898\n",
      "------------------------------------------------------------------------------------ 17\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 4s 10ms/step - loss: 149.2310\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 149.2257\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 9ms/step - loss: 149.2267\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 149.2295\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 149.2271\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 171.9347\n",
      "------------------------------------------------------------------------------------ 18\n",
      "(1414, 1, 6) (1414,)\n",
      "Epoch 1/5\n",
      "38/38 [==============================] - 3s 10ms/step - loss: 153.9719\n",
      "Epoch 2/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.9697\n",
      "Epoch 3/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.9668\n",
      "Epoch 4/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.9683\n",
      "Epoch 5/5\n",
      "38/38 [==============================] - 0s 10ms/step - loss: 153.9654\n",
      "7/7 [==============================] - 1s 5ms/step - loss: 145.1740\n",
      "------------------------------------------------------------------------------------ 19\n",
      "[12.582716431979907, 12.236550557884524, 13.045072035789811, 12.282281808000207, 11.939274680952213, 12.429991943160351, 12.517640140907774, 11.967996518792447, 12.826955688561808, 12.32419336863472, 12.095909852352985, 10.848390493245775, 11.77390924735042, 12.334407931415058, 11.330524395678017, 12.136387462287352, 13.131835320203841, 12.69211438082933, 13.112386982651651, 12.048819101806071]\n",
      "EURGBP10080 ------------------------ RNN  12.282867917124213\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAXNElEQVR4nO3df7BkdXnn8c9nGAKIq6h3kswyTIYMiAHE0dzMuuriqAWh2EVDKZnRqCGWAZMQtRYr0VRFWJN/EoOksm5IICL+CvgjWYvFMcRKrKIqP4x3cGQGGRNiiJnJFDOiq5AAycw8+0efu9P0Pd19+vzoc873vl9VXfec7/me5/vc093P6T59+rQjQgCAtKxpOwEAQP0o7gCQIIo7ACSI4g4ACaK4A0CC1radgCQtLCzEpk2b2k4DAHpl165d34qIdXnLOlHcN23apKWlpbbTAIBesf2P45ZxWAYAEkRxB4AEUdwBIEEUdwBIEMUdABJEcQeABFHcASBBnTjPvbQDB6Tf//3j8/ZT/46brruta3FmiZ1n0vIq6zYZe155df1+7lJe9sob7dMfizXpd3H/53+Wfv3XB9Nclx5A39jS9u3S7bfXHrrfxf3Hfkw6dmz88uGCvzxdd1vX4swSO8+k5VXWbTL2vPLq+v3cpbwiVt5oz28//3w1od/FfZpxbz0BIHF8oAoACaK4A0CCKO4AkCCKOwAkiOIOAAmiuANAgijuAJAgijsAJIjiDgAJorgDQIIo7gCQIIo7ACRoanG3favtQ7b3DrVdb/uA7d3Z7dKs/UTbH7G9x/YDtt/TZPIAgHxFXrnfJumSnPYbI2JLdtuZtV0h6aSIeL6kH5V0te1NtWQKAChsanGPiHskfbtgvJB0qu21kk6R9G+Svlc+PQBAGVWOuV9j+77ssM2zsrbPSPoXSQclfVPSb0VE7o7B9lW2l2wvHT58uEIaAIBRZYv7TZI2S9qiQSG/IWvfKumopP8o6UxJ19r+4bwAEXFzRCxGxOK6detKpgEAyFOquEfEwxFxNCKOSbpFg6IuSW+Q9CcR8e8RcUjSX0harCdVAEBRpYq77fVDs5dLWj6T5puSXpn1OVXSiyXtq5IgAGB2U39D1fbtkrZJWrC9X9J1krbZ3qLBB6gPSbo66/6/JH3Y9v2SLOnDEXFf/WkDACaZWtwj4vU5zR8a0/cxDU6HBAC0iG+oAkCCKO4AkCCKOwAkiOIOAAmiuANAgijuAJAgijsAJIjiDgAJorgDQIIo7gCQIIo7ACSI4g4ACaK4A0CCKO4AkKCpl/zttAcekN761pXt9uT5PvRpOvayiOltVef7HCPP8PYsMt31fk3ngMle9jLp2mtrD9vv4r5mjfS0pz21rY4n+Lg+TRSTunMs0qeOncas803EsOeTx7Dh7Vlkelq/uuM1mWuZfpju7LMbCdvv4n7OOdIXvtB2FgDQORxzB4AEUdwBIEEUdwBIEMUdABJEcQeABFHcASBBFHcASBDFHQASRHEHgARR3AEgQRR3AEgQxR0AEkRxB4AEUdwBIEFTi7vtW20fsr13qO162wds785ulw4tu8D2X9m+3/Ye2yc3lTwAIF+RV+63Sbokp/3GiNiS3XZKku21kj4u6W0RcZ6kbZL+vaZcAQAFTS3uEXGPpG8XjHexpPsi4qvZuo9ExNEK+QEASqhyzP0a2/dlh22elbU9V1LYvtv2vbZ/adzKtq+yvWR76fDhwxXSAACMKlvcb5K0WdIWSQcl3ZC1r5X0Mkk/lf293Par8gJExM0RsRgRi+vWrSuZBgAgT6niHhEPR8TRiDgm6RZJW7NF+yXdExHfioh/lbRT0ovqSRUAUFSp4m57/dDs5ZKWz6S5W9LzbT8t+3D15ZK+Vi1FAMCs1k7rYPt2Dc56WbC9X9J1krbZ3iIpJD0k6WpJiojv2P6ApC9ny3ZGxOcayRwAMNbU4h4Rr89p/tCE/h/X4HRIAEBL+IYqACSI4g4ACaK4A0CCKO4AkCCKOwAkiOIOAAmiuANAgijuAJAgijsAJIjiDgAJorgDQIKmXlum0+69V3rFK47P22lMNz0Oyhu9jybdZ/NY1vXYo33qaEst/kteIr397SvXq6jfxX1hQXrLWwbTEcfb+zw9j3Eo9OWM3keT7rN5LKs79vKtrtijfepoSzH++vUr22rQ7+K+caN0441tZwEAncMxdwBIEMUdABJEcQeABFHcASBBFHcASBDFHQASRHEHgARR3AEgQRR3AEgQxR0AEkRxB4AE9fvaMg88cPzCYXb+1emmta3G5csmXdRo3HRb/drKYZwuXl2wa7GWH39V5+uM1aWxlufPOkt61atWbteK+l3c16yRnvGM/KvZjbblXf1ulnVSWj78ZJz0JO9av7ZyGNXVqwt2KVbec6zMfJ2xiuy027B9O8V9hR/4AekXf3Fyn0lP0mnLq6zbl9iTXtnPsmw1xBp99VW0rYl16o652rS1Y8mbP/nkRv7Ffhf3v/s76bLL2s4CSEfdO5lZD1WsxmWveIX0q7+68r6oqN/F/dxzpS9/efzyaW/DJi2vsm5fYucdtimzbDXEKnIYLK+tiXX6ksesr2ZX07Jjx47PHzmiJvS7uJ96qrS42HYWANA5nAoJAAmaWtxt32r7kO29Q23X2z5ge3d2u3RknY22H7P9riaSBgBMVuSV+22SLslpvzEitmS3nSPLPiDp81WTAwCUM/WYe0TcY3tT0YC2f0LSP0j6l/JpFfToo9KePZOSma29zDp1xmpz/Hm0tTl21bynGf4wtkqfvscap877pOn7fN7xTz5ZOu20letVVOUD1Wtsv1nSkqRrI+I7tp8u6ZclXSRp4iEZ21dJukqSNm7cWC6Dffukl7603LoA0AXbt0t33FF72LLF/SZJvyYpsr83SHqLpOs1OFzzmKe8AoqImyXdLEmLi4vlXhI897nS3XePG2C29jLr1BmrzfHn0dbm2FXbir6aL9JvNcQaVed90vR93kb8zZtXttWgVHGPiIeXp23fIumubPY/SXqd7d+UdJqkY7afiIgPVk001zOfKV18cSOhAaDPShV32+sj4mA2e7mkvZIUEf9lqM/1kh5rrLBL0sMPS5/8ZPH+s7zqWA19p32Drsr0PMaY9/To9hudnjZfV9+uj7lmzWB+zZrjtzLzZT//gKQCxd327ZK2SVqwvV/SdZK22d6iwWGZhyRd3VyKE3zzm9I73tHK0ADmoOjOoK4dSpX5sjG2bpV+9mdr33RFzpZ5fU7zhwqsd32ZhGbywhdKjzxSrO8sn/Svhr7Tvh5dZXoeY8x7enT7jU5Pm6+rbx/GjBh8vX757/Kt7vl5jDE6f+RI/TmdcEI7xb3T1q6Vnv3strMAgM7pd3H/+tfHX/J33PG6PrQ3Peayoq/WutCvrRyWLR8DHj4ePHzLa59n367lNauU1pm1/+bN0itfOds6BfS7uB87Jj322Mr2cYcp+tA+jzHLfIjWhX5VY48WnqLxhg83RUhHj65sGz4UMa2tqb5V1kd7tm+nuK/wIz8i/eVftp0FkIa6dhplxk1lnTJj8GMdABplDz7cQxK45C8AJIjiDgAJorgDQIIo7gCQIIo7ACSo32fL7N0rvfGNT22b5QJHefN1xOjbfBPmMcY8x1lW11f6U4mVN79s+PsF49qa+tunsV7yEumd71Td+l3cTzpJ2rTp+HwdD8qmH/jzmJ/lidyEeX0pZl7/S5Wd57RldcWqM6865pfvm7zHYpN/5zFG3f/XGWeoCf0u7mefLX32s21nAQCd0+/i/uij0le+cny+yNulebS1Pf4sbcwXn8/7O8vb70mPE6Bm/S7u+/ZJL39521kA9Si6s6hjh9JEn0l92+jflzEvvFB697tVt34X93POkf7szwbTRY6FzaOt7fFnaWO++HzV47vz7NO1MdvoP88xh29lYjz6qJrQ7+L+jGc0cjU1AOi7fhf3xx8f/NRenrzjmeOOcdK3WBuxpku1b10xZ3mcopJ+F/fduwfniALAsrp3LrOuM2usK66QPvax8euU1O/invdDHQBWt9HPTYoua8uDDzYStt/F/QUvkG666alt095mzzJfZ6wuxR417UPFuvo0FXeeffI+pJ5n376NOasq67e1btX1zzuv2thj9Lu4f//3S297W9tZAEDncOEwAEgQxR0AEkRxB4AEUdwBIEEUdwBIUL/Pltm3T7r66uPz4y7ak/qysuuPavOLHF0ev6vfnuxbXim21xHjggukHTvy+1fQ7+IuSWuyNx95F/JZNu6CPiksK7v+qDJf/JhHrLbH7+KXXqT+5ZVie12xr7iC4r7C854nffGLbWcBAJ3DMXcASNDU4m77VtuHbO8darve9gHbu7PbpVn7RbZ32d6T/eV6vADQgiKv3G+TdElO+40RsSW77czaviXpsoh4vqSfllT/pc4AAFNNPeYeEffY3lQkWER8ZWj2fkmn2D4pIp4smR8AoIQqH6heY/vNkpYkXRsR3xlZ/lpJ9zZa2J94QjpwYPzytk+T6/r4s/wts07dsbB6jf6cXRu3Y8eaifuc50hnn137Jitb3G+S9GuSIvt7g6S3LC+0fZ6k35B08bgAtq+SdJUkbdy4sVwWe/ZIW7eWWxf91dROZ1rMKrm2sX5b69Zd/FK3fbt0xx21hy1V3CPi4eVp27dIumtofoOk/y3pzRHx9xNi3CzpZklaXFwsdw+eeab00Y+OG2C29jLr9Pl87ln+llmnb7GmxSyjamFqa+yq6y7vIFO7rVnTTNz168tv7wlKFXfb6yPiYDZ7uaS9Wftpkj4n6d0R8Re1ZDjJwoL0pjc1PgwA9M3U4m77dknbJC3Y3i/pOknbbG/R4LDMQ5KWrwFwjaSzJL3X9nuztosj4lC9aQMAJnF04JjW4uJiLC0ttZ0GAPSK7V0RsZi3jG+oAkCC+n1tmT17ql9wp49nI9Q99vD8pGWz9K0rThfGRHGjH06PnvVSpK3JGF3oO9r+kz8pfeIT9d0HmX4X91NOkc49t/z6fTwboe6xh+cnLZulb11x6h5z3Bkw0+JS6Gczelrp8Jkmo+3j2mZt73PfCy6ob9sP6XdxP+ss6dOfbjsLAOgcjrkDQIIo7gCQIIo7ACSI4g4ACaK4A0CCKO4AkKB+nwr55JPSwYMr2/POSy7bVmesrsZv8ss/XY3VN9O+21Dkuw/ziDFs3DnfmIt+F/f77uN67qjHpB3FNHUUzdWmiS8y9bX9oouk972v9k3c7+J+5pnShz/81La8J1LZtjpjdTV+U98mTSlWkWI/rc88YnQlT6n6JQVWU/tJJ03fniX0u7gvLEhXXtl2FgDQOXygCgAJorgDQIIo7gCQIIo7ACSI4g4ACaK4A0CCKO4AkCCKOwAkqN9fYjp6VHr88cF03td6y0wDQAL6XdzvvbeZa8vUtaMoOz2v8Yb/36LzVdZNJVYVqynWtOus1NWnydjzyPGCC6QdO6pt8xz9Lu4bNkjvf3/+NRuqTNcZq6tjL+vSNVz6EKuK1RQrQjp2rNhjvEqfJmPPK8crrqC4r7B+vfSud7WdBQB0Dh+oAkCCKO4AkCCKOwAkiOIOAAmiuANAgijuAJCgqcXd9q22D9neO9R2ve0Dtndnt0uHlr3H9oO2v277x5tKHAAwXpFX7rdJuiSn/caI2JLddkqS7XMl7ZB0XrbO79o+oa5kAQDFTC3uEXGPpG8XjPcaSXdExJMR8Q+SHpTUwPUBAACTVDnmfo3t+7LDNs/K2k6X9E9DffZnbSvYvsr2ku2lw4cPV0gDADCqbHG/SdJmSVskHZR0w6wBIuLmiFiMiMV169aVTAMAkKdUcY+IhyPiaEQck3SLjh96OSDpjKGuG7I2AMAclSruttcPzV4uaflMmjsl7bB9ku0zJZ0t6W+qpQgAmNXUq0Lavl3SNkkLtvdLuk7SNttbJIWkhyRdLUkRcb/tT0n6mqQjkn4hIo42kjkAYCxHnddyLmlxcTGWlpbaTgMAesX2rohYzFvGN1QBIEEUdwBIUL9/iemRR6Q//dOV7Xm/B1m2rc5YXY5fdHmVdZuM3WRew/IOY1b9mb86YnQlL8zu9NMb+S3ofhf3b3xDesMb2s4CAMrbvl26447aw/a7uJ9/vrRv31Pbirz6KNpWZ6wuxy+6vMq6TcZuOq8y74Zmna8jRlfywmxOO62RsP0u7qecIp1zTttZAEDn8IEqACSI4g4ACaK4A0CC+n3Mfdcu6cILj88Pf7CTNz1t+Sx964w1775lVT3trcr6XVq3SlsTMbvUNsm4x2Cf2+uIcdll0gc/mN+/gn4X9yeekDZvHkzP8kQap8mzLupYXiXGcvvw2R+TdhLjlk2bHxdvtP8sOVRZd9a/05bl/V+ztjURs0tty+3jzqoZt12nrWMffxwXjT/u8Z53FpRUfYdcpv288/LbK+p3cT/1VOnbRX8kKkeXXg3Oc+zR27Fjxdry2oG+W7Pm+M7CrjZfZt3nPKeRf6vfxX3LFmn//razWN3ydgCz7Bz60LfodqijTwqxJm3XKvN1xmoy9qzrbthQbPvPqN/FHe3Le3sMoHWcLQMACaK4A0CCKO4AkCCKOwAkiOIOAAmiuANAgijuAJAgijsAJMjRga+Q2z4s6R8rhFiQ9K2a0qkTec2GvGZDXrNJMa8fioh1eQs6Udyrsr0UEYtt5zGKvGZDXrMhr9mstrw4LAMACaK4A0CCUinuN7edwBjkNRvymg15zWZV5ZXEMXcAwFOl8sodADCE4g4ACepNcbd9ie2v237Q9rtzlp9k+5PZ8i/Z3tSRvK60fdj27uz21jnldavtQ7b3jllu27+T5X2f7Rd1JK9ttr87tL3eO6e8zrD9Rdtfs32/7Xfk9Jn7NiuY19y3me2Tbf+N7a9mef2PnD5zf04WzKut5+QJtr9i+66cZfVvq4jo/E3SCZL+XtIPS/o+SV+VdO5In5+X9HvZ9A5Jn+xIXldK+mAL2+xCSS+StHfM8kslfV6SJb1Y0pc6ktc2SXe1sL3WS3pRNv0fJP1tzn05921WMK+5b7NsGzw9mz5R0pckvXikTxvPySJ5tfWc/O+S/jDvvmpiW/XllftWSQ9GxDci4t8k3SHpNSN9XiPpI9n0ZyS9ym7899+K5NWKiLhH0qRfD3+NpI/GwF9LOs32+g7k1YqIOBgR92bTj0p6QNLpI93mvs0K5jV32TZ4LJs9MbuNnp0x9+dkwbzmzvYGSf9V0h+M6VL7tupLcT9d0j8Nze/Xygf4/+8TEUckfVdSMz8rPltekvTa7G38Z2yf0XBORRXNvQ3/OXtb/Xnb58178Owt8Qs1eNU3rNVtNiEvqYVtlh1m2C3pkKQvRMTY7TXH52SRvKT5Pyd/W9IvSRr3i+u1b6u+FPc++z+SNkXEBZK+oON7Z+S7V4PrZbxA0v+U9Nl5Dm776ZL+SNI7I+J78xx7kil5tbLNIuJoRGyRtEHSVtvnz2PcaQrkNdfnpO3/JulQROxqcpxRfSnuByQN7103ZG25fWyvlfRMSY+0nVdEPBIRT2azfyDpRxvOqagi23TuIuJ7y2+rI2KnpBNtL8xjbNsnalBAPxERf5zTpZVtNi2vNrdZNub/lfRFSZeMLGrjOTk1rxaeky+V9GrbD2lw6PaVtj8+0qf2bdWX4v5lSWfbPtP292nwgcOdI33ulPTT2fTrJP15ZJ9OtJnXyDHZV2twzLQL7pT05uwMkBdL+m5EHGw7Kds/uHys0fZWDR6jjReEbMwPSXogIj4wptvct1mRvNrYZrbX2T4tmz5F0kWS9o10m/tzskhe835ORsR7ImJDRGzSoEb8eUS8caRb7dtqbZWV5yUijti+RtLdGpyhcmtE3G/7fZKWIuJODZ4AH7P9oAYf2O3oSF5vt/1qSUeyvK5sOi9Jsn27BmdRLNjeL+k6DT5cUkT8nqSdGpz98aCkf5X0Mx3J63WSfs72EUmPS9oxh520NHh19SZJe7LjtZL0K5I2DuXWxjYrklcb22y9pI/YPkGDncmnIuKutp+TBfNq5Tk5qultxeUHACBBfTksAwCYAcUdABJEcQeABFHcASBBFHcASBDFHQASRHEHgAT9PwiBeYFADbSeAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for q in currency_list:\n",
    "    \n",
    "    \n",
    "    \n",
    "    sequences = [1]\n",
    "    \n",
    "    all_sequence_result = []\n",
    "        \n",
    "    for m in sequences:\n",
    "    \n",
    "        errors = []\n",
    "        for x in range(20):\n",
    "        \n",
    "            currency = q.replace('10080','')\n",
    "\n",
    "            data = pd.read_excel('files/currency_training_data/' + currency +'_combine_data_dataframe.xlsx', sheet_name=0)\n",
    "            #data = data.head(695)\n",
    "\n",
    "\n",
    "            X = data.drop(columns=['Unnamed: 0', \n",
    "                                   'date_start',  'nextweek_class',\n",
    "\n",
    "\n",
    "                                  ])\n",
    "\n",
    "\n",
    "\n",
    "            y = data['nextweek_class']\n",
    "\n",
    "\n",
    "#             X_scaler = RobustScaler(quantile_range=(10.0, 90.0),)\n",
    "            \n",
    "#             X_scaler.fit(X)\n",
    "           \n",
    "#             X = X_scaler.transform(X)\n",
    "           \n",
    "#             X = pd.DataFrame(X, columns = [q+\"_class\", q+'_volume'])\n",
    "            \n",
    "\n",
    "            #print(X)\n",
    "\n",
    "\n",
    "            # after scaling the df, resulted in \"scaled_dataset\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            result = []\n",
    "            # for loop will walk for each of the 1500 rows\n",
    "            for i in range(0,len(X)):\n",
    "                # every group must have the same length, so if current loop position i + number \n",
    "                # of sequences is higher than df length, breaks\n",
    "                if i+m <= len(X):\n",
    "                    # this will add into the list as [[R1a,R1b...R1t],[R2a,R2b...R2t],...[R5a,R5b...R5t]]\n",
    "                    result.append(X[i:i+m].values)\n",
    "            # Converting to array + keras takes float32 better than 64\n",
    "            train_x = np.array(result)\n",
    "            #train_x  = train_x.astype('float32')\n",
    "            # making the y into same length as X\n",
    "            train_y = np.array(y.head(len(train_x)).values)\n",
    "\n",
    "            print(train_x.shape, train_y.shape)\n",
    "            #print(train_x[len(train_x)-10])\n",
    "            #print(train_y[len(train_x)-10])\n",
    "            \n",
    "            \n",
    "           \n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.15 )\n",
    "                               \n",
    "            \n",
    "\n",
    "            #X_train, X_val, y_train, y_val = train_test_split( X_train, y_train, test_size = 0.15 )\n",
    "\n",
    "            \n",
    "\n",
    "            #Initializing the classifier Network\n",
    "            classifier = Sequential()\n",
    "\n",
    "            #Adding the input LSTM network layer\n",
    "            #classifier.add(CuDNNLSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "            classifier.add(LSTM(100, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "           \n",
    "           \n",
    "            classifier.add(LSTM(100,  return_sequences=True), )\n",
    "            #classifier.add(LSTM(100,  return_sequences=True))\n",
    "            \n",
    "\n",
    "            #classifier.add(Dense(units = 1))\n",
    "            classifier.add(LSTM(100,  return_sequences=False))\n",
    "            #classifier.add(Dropout(0.2))\n",
    "            #Adding a second LSTM network layer\n",
    "\n",
    "            #classifier.add(LSTM(128))\n",
    "            #Adding a dense hidden layer\n",
    "            #classifier.add(Dense(64, activation='relu'))\n",
    "            #classifier.add(Dropout(0.2))\n",
    "\n",
    "            #Adding the output layer\n",
    "            #classifier.add(Dense(35, activation='softmax'))\n",
    "\n",
    "            #Compiling the network\n",
    "            classifier.compile( loss='mean_squared_error',\n",
    "                            optimizer=Adam(learning_rate=0.001, decay=1e-6),\n",
    "                             )\n",
    "\n",
    "            #print(classifier.summary())\n",
    "\n",
    "            #Fitting the data to the model\n",
    "            history = classifier.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=5,\n",
    "                        #validation_data=(X_val, y_val)\n",
    "                                    )        \n",
    "\n",
    "            val_loss  = classifier.evaluate(X_test, y_test)\n",
    "            error = sqrt(val_loss)\n",
    "            errors.append(error)\n",
    "            plt.plot(history.history['loss'],'red')\n",
    "            #plt.plot(history.history['val_loss'], 'blue')\n",
    "            print('------------------------------------------------------------------------------------',x)\n",
    "        average_error = sum(errors)/len(errors)\n",
    "        print(errors)\n",
    "        print(q , \"------------------------ RNN \" , average_error)\n",
    "        all_sequence_result.append(str(m)+\" sequence\" )\n",
    "        all_sequence_result.append(average_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a49ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c4a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "id": "65668a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 sequence', 12.282867917124213]\n"
     ]
    }
   ],
   "source": [
    "print(all_sequence_result)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ea025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798afda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86036c13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
