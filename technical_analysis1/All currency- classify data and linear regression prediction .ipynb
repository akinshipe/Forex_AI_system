{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "37ccab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "#from keras.datasets import mnist\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fde693da",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_list = ['USDCHF1440',\n",
    "                 #'GBPUSD1440',\n",
    "    #'EURUSD1440', \n",
    "    #'USDJPY1440', \n",
    "    #'USDCAD1440', \n",
    "    #'AUDUSD1440', \n",
    "    #'NZDUSD1440',\n",
    "                 #'GBPCHF1440',\n",
    "    #'EURCHF1440', \n",
    "    #'CHFJPY1440', \n",
    "    #'CADCHF1440',\n",
    "    #'AUDCHF1440', \n",
    "    #'NZDCHF1440', \n",
    "    #'EURGBP1440',\n",
    "             #   'GBPCAD1440',\n",
    "     #'GBPAUD1440', \n",
    "    #'EURJPY1440',\n",
    "    #'EURCAD1440',\n",
    "    #'EURAUD1440',\n",
    "    #'EURNZD1440',\n",
    "    #'CADJPY1440', \n",
    "    #'AUDJPY1440',\n",
    "    #'NZDJPY1440',\n",
    "    #'AUDCAD1440', \n",
    "    #'NZDCAD1440', \n",
    "                #'AUDNZD1440'\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "# for q in currency_list:\n",
    "    \n",
    "#     errors = []\n",
    "    \n",
    "#     for x in range(5):\n",
    "\n",
    "#         currency = q.replace('1440','')\n",
    "\n",
    "#         data = pd.read_excel('files/currency_training_data/' + currency +'_combine_data_dataframe.xlsx', sheet_name=0)\n",
    "#         #data = data.head(695)\n",
    "\n",
    "\n",
    "#         X = data.drop(columns=['Unnamed: 0', \n",
    "#                                'date_start',  'nextweek_class',\n",
    "\n",
    "\n",
    "#                               ])\n",
    "\n",
    "\n",
    "\n",
    "#         y = data['nextweek_class']\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 )\n",
    "\n",
    "\n",
    "\n",
    "#         lnr= LinearRegression()\n",
    "#         lnr.fit(X_train, y_train)\n",
    "#         y_predict = lnr.predict(X_test)\n",
    "        \n",
    "        \n",
    "#         error = sqrt(mean_squared_error(y_test, y_pred))\n",
    "#         errors.append(error)\n",
    "       \n",
    "        \n",
    "#     average_error = sum(errors)/len(errors)\n",
    "       \n",
    "#     print(q + \" Linear regression Average \" + str(average_error))\n",
    "    \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "1fcc8706",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8881, 1, 3) (8881,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1, 10)             40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 10)             110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 6)              66        \n",
      "=================================================================\n",
      "Total params: 216\n",
      "Trainable params: 216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "48/48 [==============================] - 2s 12ms/step - loss: 12.7317 - mse: 12.7340 - val_loss: 12.7503 - val_mse: 12.7477\n",
      "Epoch 2/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7251 - val_loss: 12.7503 - val_mse: 12.7476\n",
      "Epoch 3/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7342 - val_loss: 12.7503 - val_mse: 12.7476\n",
      "Epoch 4/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7294 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 5/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7307 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 6/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7330 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 7/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7380 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 8/50\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 12.7317 - mse: 12.7311 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 9/50\n",
      "48/48 [==============================] - 0s 10ms/step - loss: 12.7308 - mse: 12.7311 - val_loss: 12.7501 - val_mse: 12.7475\n",
      "Epoch 10/50\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 12.7315 - mse: 12.7369 - val_loss: 12.7501 - val_mse: 12.7474\n",
      "Epoch 11/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7313 - mse: 12.7239 - val_loss: 12.7498 - val_mse: 12.7472\n",
      "Epoch 12/50\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 12.7309 - mse: 12.7376 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 13/50\n",
      "48/48 [==============================] - 1s 15ms/step - loss: 12.7317 - mse: 12.7352 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 14/50\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 12.7317 - mse: 12.7228 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 15/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 12.7317 - mse: 12.7230 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 16/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 12.7317 - mse: 12.7276 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 17/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7318 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 18/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7318 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 19/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7326 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 20/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7314 - val_loss: 12.7501 - val_mse: 12.7475\n",
      "Epoch 21/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7314 - mse: 12.7266 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 22/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7371 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 23/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7349 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 24/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7321 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 25/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7308 - mse: 12.7288 - val_loss: 12.7501 - val_mse: 12.7475\n",
      "Epoch 26/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7323 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 27/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7427 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 28/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7313 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 29/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7347 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 30/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7374 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 31/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7378 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 32/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7338 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 33/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7351 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 34/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7314 - mse: 12.7342 - val_loss: 12.7501 - val_mse: 12.7475\n",
      "Epoch 35/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7293 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 36/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7290 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 37/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7289 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 38/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7337 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 39/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7357 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 40/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7263 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 41/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7266 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 42/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7365 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 43/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7323 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 44/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 12.7317 - mse: 12.7308 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 45/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7288 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 46/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 12.7317 - mse: 12.7315 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 47/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.7317 - mse: 12.7316 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 48/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7294 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 49/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7330 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "Epoch 50/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.7317 - mse: 12.7341 - val_loss: 12.7502 - val_mse: 12.7476\n",
      "------------------------------------------------------------------------------------ 0\n",
      "(8881, 1, 3) (8881,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1, 10)             40        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 10)             110       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 6)              66        \n",
      "=================================================================\n",
      "Total params: 216\n",
      "Trainable params: 216\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 2s 15ms/step - loss: 12.6727 - mse: 12.6667 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 2/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 12.6717 - mse: 12.6718 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 3/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6748 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 4/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6772 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 5/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6737 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 6/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6674 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 7/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6740 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 8/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6762 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 9/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6718 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 10/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6765 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 11/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6735 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 12/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6717 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 13/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6697 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 14/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6736 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 15/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6716 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 16/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6693 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 17/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6776 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 18/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6713 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 19/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6728 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 20/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 12.6722 - mse: 12.6665 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 21/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6721 - val_loss: 12.8169 - val_mse: 12.8130\n",
      "Epoch 22/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6743 - val_loss: 12.8169 - val_mse: 12.8130\n",
      "Epoch 23/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6780 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 24/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6768 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 25/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6646 - val_loss: 12.8169 - val_mse: 12.8129\n",
      "Epoch 26/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6746 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 27/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6736 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 28/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6726 - mse: 12.6662 - val_loss: 12.8166 - val_mse: 12.8126\n",
      "Epoch 29/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6722 - mse: 12.6742 - val_loss: 12.8167 - val_mse: 12.8128\n",
      "Epoch 30/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6726 - mse: 12.6733 - val_loss: 12.8166 - val_mse: 12.8127\n",
      "Epoch 31/50\n",
      "48/48 [==============================] - 0s 8ms/step - loss: 12.6717 - mse: 12.6714 - val_loss: 12.8168 - val_mse: 12.8129\n",
      "Epoch 32/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6783 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 33/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6743 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 34/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6744 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 35/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6739 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 36/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6746 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 37/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6707 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 38/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6687 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 39/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6682 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 40/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6781 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 41/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6675 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 42/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6726 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 43/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6738 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 44/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6621 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 45/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6691 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 46/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6756 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 47/50\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 12.6727 - mse: 12.6708 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 48/50\n",
      "48/48 [==============================] - 0s 9ms/step - loss: 12.6727 - mse: 12.6760 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 49/50\n",
      "48/48 [==============================] - 0s 7ms/step - loss: 12.6727 - mse: 12.6775 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "Epoch 50/50\n",
      "48/48 [==============================] - 0s 6ms/step - loss: 12.6727 - mse: 12.6725 - val_loss: 12.8170 - val_mse: 12.8130\n",
      "------------------------------------------------------------------------------------ 1\n",
      "[]\n",
      "USDCHF1440 ------------------------ RNN  11.038594784938683\n"
     ]
    }
   ],
   "source": [
    "for q in currency_list:\n",
    "    \n",
    "    \n",
    "    \n",
    "    sequences = [1,]\n",
    "    \n",
    "    all_sequence_result = []\n",
    "        \n",
    "    for m in sequences:\n",
    "    \n",
    "        errors = []\n",
    "        for x in range(2):\n",
    "        \n",
    "            currency = q.replace('1440','')\n",
    "\n",
    "            data = pd.read_excel('files/currency_training_data/' + q +'_combine_data_dataframe.xlsx', sheet_name=0)\n",
    "            #data = data.head(695)\n",
    "\n",
    "\n",
    "            X = data.drop(columns=['Unnamed: 0', \n",
    "                                   'date_start',  'nextweek_class',\n",
    "\n",
    "\n",
    "                                  ])\n",
    "\n",
    "\n",
    "\n",
    "            y = data['nextweek_class']\n",
    "\n",
    "\n",
    "#             X_scaler = RobustScaler(quantile_range=(10.0, 90.0),)\n",
    "            \n",
    "#             X_scaler.fit(X)\n",
    "           \n",
    "#             X = X_scaler.transform(X)\n",
    "           \n",
    "#             X = pd.DataFrame(X, columns = [q+\"_class\", q+'_volume'])\n",
    "            \n",
    "\n",
    "            #print(X)\n",
    "\n",
    "\n",
    "            # after scaling the df, resulted in \"scaled_dataset\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            result = []\n",
    "            # for loop will walk for each of the 1500 rows\n",
    "            for i in range(0,len(X)):\n",
    "                # every group must have the same length, so if current loop position i + number \n",
    "                # of sequences is higher than df length, breaks\n",
    "                if i+m <= len(X):\n",
    "                    # this will add into the list as [[R1a,R1b...R1t],[R2a,R2b...R2t],...[R5a,R5b...R5t]]\n",
    "                    result.append(X[i:i+m].values)\n",
    "            # Converting to array + keras takes float32 better than 64\n",
    "            train_x = np.array(result)\n",
    "            #train_x  = train_x.astype('float32')\n",
    "            # making the y into same length as X\n",
    "            train_y = np.array(y.head(len(train_x)).values)\n",
    "\n",
    "            print(train_x.shape, train_y.shape)\n",
    "            #print(train_x[len(train_x)-10])\n",
    "            #print(train_y[len(train_x)-10])\n",
    "            \n",
    "            \n",
    "            #train_x = tf.keras.utils.to_categorical(train_x,)\n",
    "            #tain_y = tf.keras.utils.to_categorical(train_y)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.01 )\n",
    "             \n",
    "            \n",
    "\n",
    "            #X_train, X_val, y_train, y_val = train_test_split( X_train, y_train, test_size = 0.15 )\n",
    "\n",
    "            tf.keras.backend.clear_session()\n",
    "\n",
    "            #Initializing the classifier Network\n",
    "            classifier = Sequential()\n",
    "\n",
    "            #Adding the input LSTM network layer\n",
    "            #classifier.add(CuDNNLSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "            classifier.add(Dense(10, input_shape=(train_x.shape[1:]), \n",
    "                                 #return_sequences=True,\n",
    "                                 activation='relu'))\n",
    "           \n",
    "           \n",
    "            classifier.add(Dense(10,  \n",
    "                                 #return_sequences=True, \n",
    "                                 activation='relu')  )\n",
    "            #classifier.add(LSTM(100,  return_sequences=True))\n",
    "            \n",
    "\n",
    "            #classifier.add(Dense(units = 1))\n",
    "            classifier.add(Dense(6,  \n",
    "                                 #return_sequences=False,\n",
    "                                 activation='softmax'))\n",
    "            #classifier.add(Dropout(0.2))\n",
    "            #Adding a second LSTM network layer\n",
    "\n",
    "            #classifier.add(LSTM(128))\n",
    "            #Adding a dense hidden layer\n",
    "            #classifier.add(Dense(64, activation='relu'))\n",
    "            #classifier.add(Dropout(0.2))\n",
    "\n",
    "            #Adding the output layer\n",
    "            #classifier.add(Dense(35, activation='softmax'))\n",
    "\n",
    "            #Compiling the network\n",
    "            classifier.compile( loss='mean_squared_error',\n",
    "                            optimizer=Adam(learning_rate=0.001, decay=1e-6),\n",
    "                               metrics=['mse']\n",
    "                             )\n",
    "\n",
    "            print(classifier.summary())\n",
    "\n",
    "            #Fitting the data to the model\n",
    "            history = classifier.fit(X_train,\n",
    "                        y_train,\n",
    "                        epochs=50,\n",
    "                        validation_split=0.5,\n",
    "                        batch_size=92\n",
    "                                    )        \n",
    "\n",
    "            #val_loss  = classifier.evaluate(X_test, y_test)\n",
    "            #error = sqrt(val_loss)\n",
    "            #errors.append(error)\n",
    "            #plt.plot(history.history['loss'],'red')\n",
    "            #plt.plot(history.history['val_loss'], 'blue')\n",
    "            print('------------------------------------------------------------------------------------',x)\n",
    "            \n",
    "        \n",
    "        #average_error = sum(errors)/len(errors)\n",
    "        print(errors)\n",
    "        print(q , \"------------------------ RNN \" , average_error)\n",
    "        all_sequence_result.append(str(m)+\" sequence\" )\n",
    "        all_sequence_result.append(average_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16239ee4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "65668a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 sequence', 11.038594784938683]\n",
      "[[11960     3     5]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         2.3836004e-34, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         8.9296629e-19, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         8.3081177e-23, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         8.9467268e-38, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[1.5464945e-10, 2.0455296e-27, 2.4620347e-23, 6.3938108e-22,\n",
       "         2.6436360e-04, 9.9973565e-01]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         1.9623854e-36, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         1.9149481e-19, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         5.4370728e-38, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         2.2354375e-35, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         9.6102191e-26, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[6.0705918e-36, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         1.7301221e-13, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         1.3776016e-28, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         3.4296196e-36, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]],\n",
       "\n",
       "       [[0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "         0.0000000e+00, 1.0000000e+00]]], dtype=float32)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(all_sequence_result)   \n",
    "print(X_test[1])\n",
    "classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "4a15857f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 2 5 3 2 3 5 3 3 4 6 5 3 4 2 2 2 5 3 3 5 2 5 3 4 3 2 5 5 6 4 2 4 3 4 5 5\n",
      " 3 3 3 3 2 3 5 2 6 5 1 5 2 4 5 5 3 4 5 3 4 3 2 3 3 2 5 3 5 5 2 2 3 5 5 3 3\n",
      " 5 2 5 2 3 4 2 4 3 2 5 3 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe4363c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
