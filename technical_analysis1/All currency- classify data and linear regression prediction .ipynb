{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "37ccab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import CuDNNLSTM, Dense, Dropout, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from keras.datasets import mnist\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "fde693da",
   "metadata": {},
   "outputs": [],
   "source": [
    "currency_list = [#'USDCHF10080',\n",
    "                 #'GBPUSD10080',\n",
    "    'EURUSD10080',\n",
    "    #'USDJPY10080', 'USDCAD10080', 'AUDUSD10080', 'NZDUSD10080',\n",
    "                 #'GBPCHF10080', 'EURCHF10080', 'CHFJPY10080', 'CADCHF10080', 'AUDCHF10080', 'NZDCHF10080', 'EURGBP10080',\n",
    "                 #'GBPJPY10080', 'GBPCAD10080', 'GBPAUD10080', 'EURJPY10080', 'EURCAD10080', 'EURAUD10080', 'EURNZD10080',\n",
    "                #'CADJPY10080', 'AUDJPY10080', 'NZDJPY10080', 'AUDCAD10080', 'NZDCAD10080', \n",
    "                #'AUDNZD10080'\n",
    "                ]\n",
    "\n",
    "\n",
    "\n",
    "# for q in currency_list:\n",
    "    \n",
    "#     errors = []\n",
    "    \n",
    "#     for x in range(5):\n",
    "\n",
    "#         currency = q.replace('10080','')\n",
    "\n",
    "#         data = pd.read_excel('files/currency_training_data/' + currency +'_combine_data_dataframe.xlsx', sheet_name=0)\n",
    "#         #data = data.head(695)\n",
    "\n",
    "\n",
    "#         X = data.drop(columns=['Unnamed: 0', \n",
    "#                                'date_start',  'nextweek_class',\n",
    "\n",
    "\n",
    "#                               ])\n",
    "\n",
    "\n",
    "\n",
    "#         y = data['nextweek_class']\n",
    "\n",
    "#         X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 )\n",
    "\n",
    "\n",
    "\n",
    "#         lnr= LinearRegression()\n",
    "#         lnr.fit(X_train, y_train)\n",
    "#         y_predict = lnr.predict(X_test)\n",
    "        \n",
    "        \n",
    "#         error = sqrt(mean_squared_error(y_test, y_pred))\n",
    "#         errors.append(error)\n",
    "       \n",
    "        \n",
    "#     average_error = sum(errors)/len(errors)\n",
    "       \n",
    "#     print(q + \" Linear regression Average \" + str(average_error))\n",
    "    \n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc8706",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2558, 25, 1) (2558,)\n",
      "Model: \"sequential_119\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_256 (LSTM)              (None, 25, 100)           40800     \n",
      "_________________________________________________________________\n",
      "lstm_257 (LSTM)              (None, 1)                 408       \n",
      "=================================================================\n",
      "Total params: 41,208\n",
      "Trainable params: 41,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "58/58 [==============================] - 13s 143ms/step - loss: 254.4910 - val_loss: 266.6608\n",
      "Epoch 2/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 254.2301 - val_loss: 266.9303\n",
      "Epoch 3/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 254.0619 - val_loss: 266.7195\n",
      "Epoch 4/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 253.9985 - val_loss: 266.9561\n",
      "Epoch 5/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 253.9260 - val_loss: 266.9533\n",
      "Epoch 6/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 253.7815 - val_loss: 267.1161\n",
      "Epoch 7/20\n",
      "58/58 [==============================] - 8s 131ms/step - loss: 253.6295 - val_loss: 266.5020\n",
      "Epoch 8/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 253.5029 - val_loss: 266.5197\n",
      "Epoch 9/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 253.3121 - val_loss: 266.9348\n",
      "Epoch 10/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 253.1076 - val_loss: 266.6215\n",
      "Epoch 11/20\n",
      "58/58 [==============================] - 8s 132ms/step - loss: 252.8378 - val_loss: 266.8238\n",
      "Epoch 12/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 252.5963 - val_loss: 266.7316\n",
      "Epoch 13/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 252.3618 - val_loss: 266.5805\n",
      "Epoch 14/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 252.0327 - val_loss: 266.2042\n",
      "Epoch 15/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 251.8684 - val_loss: 265.9676\n",
      "Epoch 16/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 251.3426 - val_loss: 266.0980\n",
      "Epoch 17/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 251.0052 - val_loss: 265.5932\n",
      "Epoch 18/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 250.7471 - val_loss: 266.8434\n",
      "Epoch 19/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 250.3518 - val_loss: 265.9830\n",
      "Epoch 20/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 250.1539 - val_loss: 266.3778\n",
      "12/12 [==============================] - 1s 39ms/step - loss: 259.3582\n",
      "(2558, 25, 1) (2558,)\n",
      "Model: \"sequential_120\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_258 (LSTM)              (None, 25, 100)           40800     \n",
      "_________________________________________________________________\n",
      "lstm_259 (LSTM)              (None, 1)                 408       \n",
      "=================================================================\n",
      "Total params: 41,208\n",
      "Trainable params: 41,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "58/58 [==============================] - 14s 159ms/step - loss: 257.9427 - val_loss: 238.6887\n",
      "Epoch 2/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 257.6831 - val_loss: 238.5043\n",
      "Epoch 3/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 257.4904 - val_loss: 238.4214\n",
      "Epoch 4/20\n",
      "58/58 [==============================] - 7s 128ms/step - loss: 257.4391 - val_loss: 238.4999\n",
      "Epoch 5/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 257.3905 - val_loss: 238.3210\n",
      "Epoch 6/20\n",
      "58/58 [==============================] - 8s 132ms/step - loss: 257.2305 - val_loss: 238.3313\n",
      "Epoch 7/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 257.2435 - val_loss: 238.4579\n",
      "Epoch 8/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 256.7661 - val_loss: 238.0844\n",
      "Epoch 9/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 256.4689 - val_loss: 237.8429\n",
      "Epoch 10/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 256.1653 - val_loss: 237.9875\n",
      "Epoch 11/20\n",
      "58/58 [==============================] - 8s 146ms/step - loss: 255.8129 - val_loss: 236.8798\n",
      "Epoch 12/20\n",
      "58/58 [==============================] - 9s 156ms/step - loss: 255.7676 - val_loss: 237.8794\n",
      "Epoch 13/20\n",
      "58/58 [==============================] - 8s 130ms/step - loss: 255.5050 - val_loss: 238.8570\n",
      "Epoch 14/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 254.7914 - val_loss: 237.6854\n",
      "Epoch 15/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 255.1483 - val_loss: 237.4720\n",
      "Epoch 16/20\n",
      "58/58 [==============================] - 8s 134ms/step - loss: 254.5775 - val_loss: 237.6500\n",
      "Epoch 17/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 253.7457 - val_loss: 238.1882\n",
      "Epoch 18/20\n",
      "58/58 [==============================] - 7s 127ms/step - loss: 253.1406 - val_loss: 238.2597\n",
      "Epoch 19/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 252.7427 - val_loss: 238.1097\n",
      "Epoch 20/20\n",
      "58/58 [==============================] - 7s 123ms/step - loss: 252.5780 - val_loss: 239.1476\n",
      "12/12 [==============================] - 2s 40ms/step - loss: 266.0090\n",
      "(2558, 25, 1) (2558,)\n",
      "Model: \"sequential_121\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_260 (LSTM)              (None, 25, 100)           40800     \n",
      "_________________________________________________________________\n",
      "lstm_261 (LSTM)              (None, 1)                 408       \n",
      "=================================================================\n",
      "Total params: 41,208\n",
      "Trainable params: 41,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "58/58 [==============================] - 12s 138ms/step - loss: 256.6772 - val_loss: 231.2696\n",
      "Epoch 2/20\n",
      "58/58 [==============================] - 7s 119ms/step - loss: 256.3745 - val_loss: 231.2493\n",
      "Epoch 3/20\n",
      "58/58 [==============================] - 7s 120ms/step - loss: 256.2523 - val_loss: 230.6062\n",
      "Epoch 4/20\n",
      "58/58 [==============================] - 7s 119ms/step - loss: 255.9558 - val_loss: 230.5288\n",
      "Epoch 5/20\n",
      "58/58 [==============================] - 8s 130ms/step - loss: 255.7679 - val_loss: 231.2301\n",
      "Epoch 6/20\n",
      "58/58 [==============================] - 8s 131ms/step - loss: 257.4889 - val_loss: 232.9049\n",
      "Epoch 7/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 257.2832 - val_loss: 231.9704\n",
      "Epoch 8/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 256.8943 - val_loss: 231.9911\n",
      "Epoch 9/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 256.8171 - val_loss: 231.8537\n",
      "Epoch 10/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 256.7617 - val_loss: 231.9088\n",
      "Epoch 11/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 256.8375 - val_loss: 231.9048\n",
      "Epoch 12/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 256.7273 - val_loss: 231.7923\n",
      "Epoch 13/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 256.6512 - val_loss: 231.6643\n",
      "Epoch 14/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 256.5839 - val_loss: 232.0435\n",
      "Epoch 15/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 256.7856 - val_loss: 232.1566\n",
      "Epoch 16/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 256.9068 - val_loss: 232.0310\n",
      "Epoch 17/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 256.8928 - val_loss: 232.0305\n",
      "Epoch 18/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 256.8923 - val_loss: 232.0304\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - 7s 122ms/step - loss: 256.8920 - val_loss: 232.0302\n",
      "Epoch 20/20\n",
      "58/58 [==============================] - 7s 122ms/step - loss: 256.8918 - val_loss: 232.0299\n",
      "12/12 [==============================] - 1s 38ms/step - loss: 278.1773\n",
      "(2558, 25, 1) (2558,)\n",
      "Model: \"sequential_122\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_262 (LSTM)              (None, 25, 100)           40800     \n",
      "_________________________________________________________________\n",
      "lstm_263 (LSTM)              (None, 1)                 408       \n",
      "=================================================================\n",
      "Total params: 41,208\n",
      "Trainable params: 41,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "58/58 [==============================] - 14s 143ms/step - loss: 253.9373 - val_loss: 262.9011\n",
      "Epoch 2/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 253.4758 - val_loss: 263.0895\n",
      "Epoch 3/20\n",
      "58/58 [==============================] - 7s 123ms/step - loss: 253.3901 - val_loss: 263.6822\n",
      "Epoch 4/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 253.0625 - val_loss: 264.1308\n",
      "Epoch 5/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 252.8845 - val_loss: 264.7782\n",
      "Epoch 6/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 252.7022 - val_loss: 265.1014\n",
      "Epoch 7/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 252.8027 - val_loss: 264.6408\n",
      "Epoch 8/20\n",
      "58/58 [==============================] - 7s 123ms/step - loss: 252.1726 - val_loss: 263.9478\n",
      "Epoch 9/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 251.7986 - val_loss: 266.4477\n",
      "Epoch 10/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 252.6490 - val_loss: 265.4838\n",
      "Epoch 11/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 251.8610 - val_loss: 264.0766\n",
      "Epoch 12/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 251.7605 - val_loss: 266.3266\n",
      "Epoch 13/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 252.3825 - val_loss: 267.1360\n",
      "Epoch 14/20\n",
      "58/58 [==============================] - 7s 123ms/step - loss: 252.8377 - val_loss: 266.8069\n",
      "Epoch 15/20\n",
      "58/58 [==============================] - 7s 123ms/step - loss: 252.6296 - val_loss: 266.5942\n",
      "Epoch 16/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 252.8135 - val_loss: 266.5218\n",
      "Epoch 17/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 252.1916 - val_loss: 265.3019\n",
      "Epoch 18/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 251.6769 - val_loss: 264.1514\n",
      "Epoch 19/20\n",
      "58/58 [==============================] - 8s 130ms/step - loss: 251.3601 - val_loss: 264.8560\n",
      "Epoch 20/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 250.9040 - val_loss: 262.8418\n",
      "12/12 [==============================] - 1s 38ms/step - loss: 267.6787\n",
      "(2558, 25, 1) (2558,)\n",
      "Model: \"sequential_123\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_264 (LSTM)              (None, 25, 100)           40800     \n",
      "_________________________________________________________________\n",
      "lstm_265 (LSTM)              (None, 1)                 408       \n",
      "=================================================================\n",
      "Total params: 41,208\n",
      "Trainable params: 41,208\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "58/58 [==============================] - 13s 142ms/step - loss: 257.6086 - val_loss: 247.9093\n",
      "Epoch 2/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 257.4569 - val_loss: 247.9168\n",
      "Epoch 3/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 257.1676 - val_loss: 248.1749\n",
      "Epoch 4/20\n",
      "58/58 [==============================] - 7s 124ms/step - loss: 257.0848 - val_loss: 248.1399\n",
      "Epoch 5/20\n",
      "58/58 [==============================] - 7s 125ms/step - loss: 256.8965 - val_loss: 248.4500\n",
      "Epoch 6/20\n",
      "58/58 [==============================] - 7s 123ms/step - loss: 256.7796 - val_loss: 248.3039\n",
      "Epoch 7/20\n",
      "58/58 [==============================] - 7s 120ms/step - loss: 256.7024 - val_loss: 248.3767\n",
      "Epoch 8/20\n",
      "58/58 [==============================] - 7s 121ms/step - loss: 256.3084 - val_loss: 248.3956\n",
      "Epoch 9/20\n",
      "58/58 [==============================] - 7s 123ms/step - loss: 256.0270 - val_loss: 248.7065\n",
      "Epoch 10/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 255.4780 - val_loss: 248.5660\n",
      "Epoch 11/20\n",
      "58/58 [==============================] - 8s 132ms/step - loss: 255.8916 - val_loss: 248.9583\n",
      "Epoch 12/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 255.0641 - val_loss: 249.2609\n",
      "Epoch 13/20\n",
      "58/58 [==============================] - 7s 126ms/step - loss: 254.8985 - val_loss: 249.6012\n",
      "Epoch 14/20\n",
      "16/58 [=======>......................] - ETA: 5s - loss: 271.9977"
     ]
    }
   ],
   "source": [
    "for q in currency_list:\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for x in range(5):\n",
    "\n",
    "        currency = q.replace('10080','')\n",
    "\n",
    "        data = pd.read_excel('files/currency_training_data/' + currency +'_combine_data_dataframe.xlsx', sheet_name=0)\n",
    "        #data = data.head(695)\n",
    "\n",
    "\n",
    "        X = data.drop(columns=['Unnamed: 0', \n",
    "                               'date_start',  'nextweek_class',\n",
    "\n",
    "\n",
    "                              ])\n",
    "\n",
    "\n",
    "\n",
    "        y = data['nextweek_class']\n",
    "      \n",
    "    \n",
    "        \n",
    "\n",
    "        #print(X.shape)\n",
    "        \n",
    "        \n",
    "        # after scaling the df, resulted in \"scaled_dataset\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        sequences = 25\n",
    "        \n",
    "        \n",
    "        \n",
    "        result = []\n",
    "        # for loop will walk for each of the 1500 rows\n",
    "        for i in range(0,len(X)):\n",
    "            # every group must have the same length, so if current loop position i + number \n",
    "            # of sequences is higher than df length, breaks\n",
    "            if i+sequences <= len(X):\n",
    "                # this will add into the list as [[R1a,R1b...R1t],[R2a,R2b...R2t],...[R5a,R5b...R5t]]\n",
    "                result.append(X[i:i+sequences].values)\n",
    "        # Converting to array + keras takes float32 better than 64\n",
    "        train_x = np.array(result)\n",
    "        #train_x  = train_x.astype('float32')\n",
    "        # making the y into same length as X\n",
    "        train_y = np.array(y.head(len(train_x)).values)\n",
    "\n",
    "        print(train_x.shape, train_y.shape)\n",
    "        #print(train_x[len(train_x)-10])\n",
    "        #print(train_y[len(train_x)-10])\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.15 )\n",
    "        \n",
    "        X_train, X_val, y_train, y_val = train_test_split( X_train, y_train, test_size = 0.15 )\n",
    "       \n",
    "        \n",
    "       \n",
    "        #Initializing the classifier Network\n",
    "        classifier = Sequential()\n",
    "\n",
    "        #Adding the input LSTM network layer\n",
    "        #classifier.add(CuDNNLSTM(128, input_shape=(X_train.shape[1:]), return_sequences=True))\n",
    "        classifier.add(LSTM(100, input_shape=(train_x.shape[1:]), return_sequences=True))\n",
    "        #classifier.add(LSTM(100,  return_sequences=True),)\n",
    "        #classifier.add(LSTM(100,  return_sequences=True), )\n",
    "        #classifier.add(LSTM(100,  return_sequences=True))\n",
    "      \n",
    "        #classifier.add(Dense(units = 1))\n",
    "        classifier.add(LSTM(1,  return_sequences=False))\n",
    "        #classifier.add(Dropout(0.2))\n",
    "        #Adding a second LSTM network layer\n",
    "        \n",
    "        #classifier.add(LSTM(128))\n",
    "        #Adding a dense hidden layer\n",
    "        #classifier.add(Dense(64, activation='relu'))\n",
    "        #classifier.add(Dropout(0.2))\n",
    "\n",
    "        #Adding the output layer\n",
    "        #classifier.add(Dense(35, activation='softmax'))\n",
    "      \n",
    "        #Compiling the network\n",
    "        classifier.compile( loss='mean_squared_error',\n",
    "                      optimizer=Adam(learning_rate=0.001, decay=1e-6),\n",
    "                      )\n",
    "        \n",
    "        print(classifier.summary())\n",
    "\n",
    "        #Fitting the data to the model\n",
    "        history = classifier.fit(X_train,\n",
    "                 y_train,\n",
    "                  epochs=20,\n",
    "                  validation_data=(X_val, y_val))        \n",
    "     \n",
    "        val_loss  = classifier.evaluate(X_test, y_test)\n",
    "        error = sqrt(val_loss)\n",
    "        errors.append(error)\n",
    "        plt.plot(history.history['loss'],'red')\n",
    "        plt.plot(history.history['val_loss'], 'blue')\n",
    "    average_error = sum(errors)/len(errors)\n",
    "    print(errors)\n",
    "    print(q , \"------------------------ RNN \" , average_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65668a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f56b9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ea025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5798afda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
